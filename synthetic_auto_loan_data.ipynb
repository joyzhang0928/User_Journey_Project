{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd4ebd3b-3101-4505-b5ca-df8be9cf1a9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faker\n",
      "  Downloading Faker-27.4.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.4 in /opt/anaconda3/lib/python3.11/site-packages (from faker) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.4->faker) (1.16.0)\n",
      "Downloading Faker-27.4.0-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: faker\n",
      "Successfully installed faker-27.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ec3d7a-fa79-4a7d-9f03-7e0d5af34ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credit Score Bias: Adjust the Interest_Rate and Application_Status based on the Credit_Score. Higher credit scores will have lower interest rates and a higher likelihood of approval.\n",
    "# Geographic Bias: Influence the Promotions field based on the Location. Certain locations will have a higher chance of receiving better promotional offers.\n",
    "# increase Sample Size: Update n_samples to 70,000.\n",
    "# Introduce Missing Data: Use probabilities to assign None values to certain fields at random, representing missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2acb6344",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from datetime import datetime, timedelta\n",
    "from faker import Faker\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Initialize Faker to generate synthetic data\n",
    "fake = Faker()\n",
    "\n",
    "# Number of samples\n",
    "num_samples = 20000\n",
    "\n",
    "# Seed for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)  # For random.sample and random.choice in non-numpy contexts\n",
    "\n",
    "def generate_correlated_features(num_samples):\n",
    "    \"\"\"\n",
    "    Generate correlated personal and financial features.\n",
    "    \"\"\"\n",
    "    # Generate Age with normal distribution, clipped between 18 and 80\n",
    "    age = np.random.normal(40, 12, num_samples).clip(18, 80).astype(int)\n",
    "    \n",
    "    # Generate Experience based on Age, ensuring non-negative\n",
    "    experience = (age - 18 - np.random.normal(4, 2, num_samples)).clip(0).astype(int)\n",
    "    \n",
    "    # Generate Education Level with predefined probabilities\n",
    "    education_level = np.random.choice(\n",
    "        ['High School', 'Associate', 'Bachelor', 'Master', 'Doctorate'], \n",
    "        num_samples, \n",
    "        p=[0.3, 0.2, 0.3, 0.15, 0.05]\n",
    "    )\n",
    "    \n",
    "    # Impact of Education on Income and Credit Score\n",
    "    edu_impact = {'High School': 0, 'Associate': 0.1, 'Bachelor': 0.2, 'Master': 0.3, 'Doctorate': 0.4}\n",
    "    edu_factor = np.array([edu_impact[level] for level in education_level])\n",
    "    \n",
    "    # Generate Annual Income using log-normal distribution influenced by education and experience\n",
    "    base_income = np.random.lognormal(10.5, 0.6, num_samples) * (1 + edu_factor) * (1 + experience / 100)\n",
    "    income_noise = np.random.normal(0, 0.1, num_samples)\n",
    "    annual_income = (base_income * (1 + income_noise)).clip(15000, 300000).astype(int)\n",
    "    \n",
    "    # Generate Credit Score influenced by education, experience, and income\n",
    "    credit_score_base = 300 + 300 * stats.beta.rvs(5, 1.5, size=num_samples)\n",
    "    credit_score = (credit_score_base + edu_factor * 100 + experience * 1.5 + income_noise * 100).clip(300, 850).astype(int)\n",
    "    \n",
    "    # Generate Employment Status probabilities based on education\n",
    "    employment_status_probs = np.column_stack([\n",
    "        0.9 - edu_factor * 0.3,  # Employed\n",
    "        0.05 + edu_factor * 0.2,  # Self-Employed\n",
    "        0.05 + edu_factor * 0.1   # Unemployed\n",
    "    ])\n",
    "    # Normalize probabilities to sum to 1\n",
    "    employment_status_probs = employment_status_probs / employment_status_probs.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    # Assign Employment Status based on probabilities\n",
    "    employment_status = np.array(['Employed', 'Self-Employed', 'Unemployed'])[\n",
    "        np.argmax(np.random.random((num_samples, 1)) < employment_status_probs.cumsum(axis=1), axis=1)\n",
    "    ]\n",
    "    \n",
    "    return age, experience, education_level, annual_income, credit_score, employment_status\n",
    "\n",
    "# def generate_time_based_features(num_samples):\n",
    "#     \"\"\"\n",
    "#     Generate sequential application dates starting from January 1, 2018.\n",
    "#     \"\"\"\n",
    "#     start_date = datetime(2018, 1, 1)\n",
    "#     dates = [start_date + timedelta(days=i) for i in range(num_samples)]\n",
    "# #     return dates\n",
    "# def generate_time_based_features(num_samples):\n",
    "#     \"\"\"\n",
    "#     Generate sequential application dates starting from January 1, 2018,\n",
    "#     ensuring dates do not exceed October 1, 2024.\n",
    "#     \"\"\"\n",
    "#     start_date = datetime(2018, 1, 1)\n",
    "#     end_date = datetime(2024, 10, 1)\n",
    "    \n",
    "#     # Generate dates and ensure they do not exceed end_date\n",
    "#     dates = [start_date + timedelta(days=i) for i in range(num_samples)]\n",
    "#     dates = [date if date <= end_date else end_date for date in dates]\n",
    "    \n",
    "#     return dates\n",
    "\n",
    "\n",
    "# Generate correlated features\n",
    "age, experience, education_level, annual_income, credit_score, employment_status = generate_correlated_features(num_samples)\n",
    "application_dates = generate_time_based_features(num_samples)\n",
    "\n",
    "# Define a dictionary mapping vehicle makes to possible models\n",
    "make_model_mapping = {\n",
    "    'Toyota': ['Camry', 'Corolla', 'RAV4', 'Prius', 'Highlander', 'Tacoma'],\n",
    "    'Honda': ['Civic', 'Accord', 'CR-V', 'Pilot', 'Fit', 'Odyssey'],\n",
    "    'Ford': ['F-150', 'Escape', 'Explorer', 'Mustang', 'Fusion', 'Ranger'],\n",
    "    'Chevrolet': ['Silverado', 'Equinox', 'Malibu', 'Traverse', 'Camaro', 'Tahoe'],\n",
    "    'BMW': ['3 Series', '5 Series', 'X3', 'X5', '7 Series', 'X1'],\n",
    "    'Mercedes-Benz': ['C-Class', 'E-Class', 'GLC', 'GLE', 'S-Class', 'GLA'],\n",
    "    'Nissan': ['Altima', 'Sentra', 'Rogue', 'Versa', 'Pathfinder', 'Maxima'],\n",
    "    'Hyundai': ['Elantra', 'Sonata', 'Tucson', 'Santa Fe', 'Accent', 'Kona'],\n",
    "    'Kia': ['Soul', 'Optima', 'Sportage', 'Sorento', 'Rio', 'Seltos'],\n",
    "    'Subaru': ['Forester', 'Outback', 'Impreza', 'Crosstrek', 'Legacy', 'Ascent'],\n",
    "    'Mazda': ['CX-5', 'Mazda3', 'Mazda6', 'MX-5 Miata', 'CX-9', 'Mazda CX-30'],\n",
    "    'Audi': ['A3', 'A4', 'A6', 'Q5', 'Q7', 'TT'],\n",
    "    'Volkswagen': ['Golf', 'Passat', 'Tiguan', 'Jetta', 'Atlas', 'Arteon'],\n",
    "    'Volvo': ['XC90', 'S60', 'S90', 'XC60', 'V60', 'V90'],\n",
    "    'Porsche': ['911', 'Cayenne', 'Macan', 'Panamera', 'Taycan', 'Boxster'],\n",
    "    'Jeep': ['Wrangler', 'Grand Cherokee', 'Renegade', 'Compass', 'Cherokee', 'Gladiator'],\n",
    "    'Lexus': ['RX', 'ES', 'NX', 'GX', 'LS', 'IS'],\n",
    "    'Acura': ['MDX', 'RDX', 'TLX', 'ILX', 'RLX', 'NSX'],\n",
    "    'Cadillac': ['Escalade', 'XT5', 'CT5', 'XT4', 'ATS', 'XT6'],\n",
    "    'Lincoln': ['Navigator', 'Aviator', 'Corsair', 'Nautilus', 'MKZ', 'MKC'],\n",
    "    'Infiniti': ['Q50', 'QX60', 'QX80', 'Q30', 'QX50', 'QX55'],\n",
    "    'Genesis': ['G70', 'G80', 'G90', 'GV70', 'GV80', 'G70 Convertible'],\n",
    "    'Bentley': ['Continental', 'Flying Spur', 'Bentayga', 'Mulsanne', 'Azure'],\n",
    "    'Maserati': ['Ghibli', 'Quattroporte', 'Levante', 'GranTurismo', 'MC20'],\n",
    "    'Alfa Romeo': ['Giulia', 'Stelvio', '4C', 'Giulietta', 'Tonale', 'GT'],\n",
    "    'Fiat': ['500', 'Panda', '124 Spider', 'Tipo', '500X', '500L'],\n",
    "    'Mitsubishi': ['Outlander', 'Eclipse Cross', 'Mirage', 'Galant', 'Lancer', 'ASX'],\n",
    "    'Mini': ['Cooper', 'Countryman', 'Clubman', 'Convertible', 'Hardtop'],\n",
    "    'Ram': ['1500', '2500', '3500', 'ProMaster', 'Chassis Cab'],\n",
    "    'Suzuki': ['Swift', 'Vitara', 'Jimny', 'Baleno', 'Celerio', 'S-Cross']\n",
    "}\n",
    "\n",
    "# Expanded list of vehicle makes with approximate weights (illustrative)\n",
    "vehicle_makes = list(make_model_mapping.keys())\n",
    "\n",
    "# Corresponding weights (adjust based on actual market data as needed)\n",
    "vehicle_make_weights = [\n",
    "    10,  # Toyota\n",
    "    9,   # Honda\n",
    "    8,   # Ford\n",
    "    7,   # Chevrolet\n",
    "    6,   # BMW\n",
    "    5,   # Mercedes-Benz\n",
    "    6,   # Nissan\n",
    "    5,   # Hyundai\n",
    "    5,   # Kia\n",
    "    4,   # Subaru\n",
    "    3,   # Mazda\n",
    "    3,   # Audi\n",
    "    2,   # Volkswagen\n",
    "    2,   # Volvo\n",
    "    1,   # Porsche\n",
    "    4,   # Jeep\n",
    "    3,   # Lexus\n",
    "    2,   # Acura\n",
    "    1,   # Cadillac\n",
    "    1,   # Lincoln\n",
    "    2,   # Infiniti\n",
    "    2,   # Genesis\n",
    "    1,   # Bentley\n",
    "    1,   # Maserati\n",
    "    1,   # Alfa Romeo\n",
    "    1,   # Fiat\n",
    "    1,   # Mitsubishi\n",
    "    1,   # Mini\n",
    "    1,   # Ram\n",
    "    1    # Suzuki\n",
    "]\n",
    "\n",
    "# Convert weights to probabilities\n",
    "total_weight = sum(vehicle_make_weights)\n",
    "vehicle_make_probabilities = [weight / total_weight for weight in vehicle_make_weights]\n",
    "\n",
    "# Generate Vehicle_Make data based on probabilities\n",
    "vehicle_make_data = np.random.choice(\n",
    "    vehicle_makes, \n",
    "    size=num_samples, \n",
    "    p=vehicle_make_probabilities\n",
    ")\n",
    "\n",
    "# Function to assign a model based on make\n",
    "def assign_model(make):\n",
    "    return random.choice(make_model_mapping.get(make, ['Model_Not_Specified']))\n",
    "\n",
    "# Generate Vehicle_Model data based on Vehicle_Make\n",
    "vehicle_model_data = [assign_model(make) for make in vehicle_make_data]\n",
    "\n",
    "# Define probabilities for Vehicle_Type\n",
    "vehicle_type_probs = [0.4, 0.6]  # 40% New, 60% Used\n",
    "\n",
    "# Generate Vehicle_Type data\n",
    "vehicle_type_data = np.random.choice(['New', 'Used'], size=num_samples, p=vehicle_type_probs)\n",
    "\n",
    "# Generate Vehicle_Year based on Vehicle_Type\n",
    "vehicle_year_data = []\n",
    "for vt in vehicle_type_data:\n",
    "    if vt == 'New':\n",
    "        # New vehicles: Recent years (e.g., 2018-2024)\n",
    "        year = np.random.randint(2018, 2025)\n",
    "    else:\n",
    "        # Used vehicles: Older years (e.g., 2005-2017)\n",
    "        year = np.random.randint(2005, 2018)\n",
    "    vehicle_year_data.append(year)\n",
    "\n",
    "# Generate Vehicle_Mileage based on Vehicle_Year and Vehicle_Type\n",
    "vehicle_mileage_data = []\n",
    "current_year = 2024\n",
    "\n",
    "for year, vt in zip(vehicle_year_data, vehicle_type_data):\n",
    "    age = current_year - year\n",
    "    if vt == 'New':\n",
    "        # New vehicles: Low mileage, e.g., 0-30,000 miles\n",
    "        mileage = np.random.randint(0, 30001)\n",
    "    else:\n",
    "        # Used vehicles: Mileage increases with age, e.g., 30,000 + (age * 12,000) +/- 10,000\n",
    "        avg_mileage = age * 12000\n",
    "        min_mileage = max(avg_mileage - 10000, 30000)\n",
    "        max_mileage = avg_mileage + 10000\n",
    "        mileage = np.random.randint(min_mileage, max_mileage + 1)\n",
    "    vehicle_mileage_data.append(mileage)\n",
    "\n",
    "# Define base prices for each Vehicle_Make (in USD)\n",
    "base_price_mapping = {\n",
    "    'Toyota': 25000,\n",
    "    'Honda': 24000,\n",
    "    'Ford': 26000,\n",
    "    'Chevrolet': 25500,\n",
    "    'BMW': 45000,\n",
    "    'Mercedes-Benz': 47000,\n",
    "    'Nissan': 23000,\n",
    "    'Hyundai': 22000,\n",
    "    'Kia': 21000,\n",
    "    'Subaru': 23500,\n",
    "    'Mazda': 22500,\n",
    "    'Audi': 44000,\n",
    "    'Volkswagen': 20000,\n",
    "    'Volvo': 42000,\n",
    "    'Porsche': 60000,\n",
    "    'Jeep': 28000,\n",
    "    'Lexus': 43000,\n",
    "    'Acura': 39000,\n",
    "    'Cadillac': 50000,\n",
    "    'Lincoln': 48000,\n",
    "    'Infiniti': 40000,\n",
    "    'Genesis': 41000,\n",
    "    'Bentley': 90000,\n",
    "    'Maserati': 85000,\n",
    "    'Alfa Romeo': 37000,\n",
    "    'Fiat': 18000,\n",
    "    'Mitsubishi': 19000,\n",
    "    'Mini': 22000,\n",
    "    'Ram': 30000,\n",
    "    'Suzuki': 17000\n",
    "}\n",
    "\n",
    "# Assign base price to each vehicle based on Vehicle_Make\n",
    "vehicle_price_data = []\n",
    "for make in vehicle_make_data:\n",
    "    base_price = base_price_mapping.get(make, 20000)  # Default base price if make not found\n",
    "    vehicle_price_data.append(base_price)\n",
    "\n",
    "# Adjust Vehicle_Price based on Vehicle_Type, Vehicle_Year, and Vehicle_Mileage\n",
    "adjusted_vehicle_price_data = []\n",
    "for i in range(num_samples):\n",
    "    make = vehicle_make_data[i]\n",
    "    base_price = base_price_mapping.get(make, 20000)\n",
    "    vt = vehicle_type_data[i]\n",
    "    year = vehicle_year_data[i]\n",
    "    mileage = vehicle_mileage_data[i]\n",
    "    \n",
    "    if vt == 'New':\n",
    "        # New vehicles: Slight adjustment for models or additional features can be added here\n",
    "        price = base_price\n",
    "    else:\n",
    "        # Used vehicles: Apply depreciation based on age and mileage\n",
    "        age = current_year - year\n",
    "        # Depreciation rate: 5% per year\n",
    "        depreciation = 0.05 * age\n",
    "        # Mileage factor: Assume higher mileage reduces price\n",
    "        mileage_factor = min(mileage / 150000, 1)  # Cap at 1\n",
    "        mileage_depreciation = 0.2 * mileage_factor  # Up to 20% depreciation based on mileage\n",
    "        \n",
    "        total_depreciation = depreciation + mileage_depreciation\n",
    "        total_depreciation = min(total_depreciation, 0.8)  # Cap total depreciation at 80%\n",
    "        \n",
    "        price = base_price * (1 - total_depreciation)\n",
    "    \n",
    "    # Add some randomness (±5%)\n",
    "    price *= np.random.uniform(0.95, 1.05)\n",
    "    \n",
    "    # Ensure price is not negative\n",
    "    price = max(price, 1000)\n",
    "    \n",
    "    adjusted_vehicle_price_data.append(int(price))\n",
    "\n",
    "# Define Loan_Amount based on Vehicle_Price and Vehicle_Type\n",
    "loan_amount_data = []\n",
    "for i in range(num_samples):\n",
    "    price = adjusted_vehicle_price_data[i]\n",
    "    vt = vehicle_type_data[i]\n",
    "    \n",
    "    if vt == 'New':\n",
    "        # New vehicles: LTV between 80% - 100%\n",
    "        ltv = np.random.uniform(0.8, 1.0)\n",
    "    else:\n",
    "        # Used vehicles: LTV between 50% - 80%\n",
    "        ltv = np.random.uniform(0.5, 0.8)\n",
    "    \n",
    "    loan_amount = price * ltv\n",
    "    \n",
    "    # Add some randomness (±5%)\n",
    "    loan_amount *= np.random.uniform(0.95, 1.05)\n",
    "    \n",
    "    # Ensure loan amount does not exceed vehicle price\n",
    "    loan_amount = min(loan_amount, price)\n",
    "    \n",
    "    # Convert to integer\n",
    "    loan_amount_data.append(int(loan_amount))\n",
    "\n",
    "# Generate Location data using Faker\n",
    "location_data = [fake.city() for _ in range(num_samples)]\n",
    "\n",
    "# Generate Down_Payment as a percentage of Vehicle_Price (10% - 30%)\n",
    "down_payment_data = []\n",
    "for price in adjusted_vehicle_price_data:\n",
    "    down_payment = np.random.randint(int(price * 0.1), int(price * 0.3) + 1)\n",
    "    down_payment_data.append(down_payment)\n",
    "\n",
    "# Generate Loan_Tenure_Years based on Loan_Amount\n",
    "loan_tenure_data = []\n",
    "for loan in loan_amount_data:\n",
    "    if loan > 30000:\n",
    "        tenure = np.random.choice([5, 6, 7], p=[0.5, 0.3, 0.2])\n",
    "    elif loan > 20000:\n",
    "        tenure = np.random.choice([4, 5, 6], p=[0.4, 0.4, 0.2])\n",
    "    else:\n",
    "        tenure = np.random.choice([3, 4, 5], p=[0.5, 0.3, 0.2])\n",
    "    loan_tenure_data.append(tenure)\n",
    "\n",
    "# Generate Interest_Rate based on Credit_Score, Loan_Amount, Loan_Tenure_Years, and Annual_Income\n",
    "interest_rate_data = []\n",
    "for credit, loan, tenure, income in zip(credit_score, loan_amount_data, loan_tenure_data, annual_income):\n",
    "    base_rate = 2.0  # Base interest rate\n",
    "    # Higher credit score reduces interest rate\n",
    "    credit_factor = (850 - credit) / 2000  # Scaled factor\n",
    "    # Higher loan amount may increase interest rate\n",
    "    loan_factor = (loan - 5000) / 100000  # Scaled factor\n",
    "    # Longer tenure may increase interest rate\n",
    "    tenure_factor = (tenure - 3) * 0.2\n",
    "    # Higher income may reduce interest rate\n",
    "    income_factor = (income - 50000) / 200000  # Scaled factor\n",
    "    \n",
    "    interest = base_rate + credit_factor + loan_factor + tenure_factor - income_factor\n",
    "    # Add some randomness\n",
    "    interest += np.random.uniform(-0.3, 0.3)\n",
    "    # Clip interest rate to realistic bounds\n",
    "    interest = min(max(interest, 1.9), 6.5)\n",
    "    interest_rate_data.append(round(interest, 2))\n",
    "\n",
    "    \n",
    "# Define additional fields to be added\n",
    "marital_statuses = ['Single', 'Married', 'Divorced', 'Widowed']\n",
    "device_types = ['iPhone', 'Android', 'Windows Phone']\n",
    "os_versions = ['iOS 15', 'iOS 14', 'Android 11', 'Android 10', 'Windows 10 Mobile']\n",
    "app_versions = ['1.0', '1.1', '1.2']\n",
    "network_types = ['Wi-Fi', '4G', '5G']\n",
    "dealer_info = ['Dealer A', 'Dealer B', 'Dealer C', 'Dealer D']\n",
    "promotions = ['0% APR', '$1000 Cashback', 'No Payments for 90 Days', 'Low Down Payment']\n",
    "event_sequences = ['Application Start', 'Vehicle Selection', 'Loan Calculator', 'Document Upload', 'Credit Check', 'Approval']\n",
    "screens = ['Home', 'Vehicle Selection','Loan Calculator', 'Document Upload', 'Credit Check', 'Approval']\n",
    "\n",
    "\n",
    "#Generate Common path and Ensure sequential navigation path generation starting with 'Home' and following the specified screen order\n",
    "def generate_sequential_navigation(screens, num_samples):\n",
    "    \"\"\"\n",
    "    Generate sequential navigation paths that always start with 'Home' \n",
    "    and follow the specified screen order without skipping forward.\n",
    "    \"\"\"\n",
    "    navigation_paths = []\n",
    "    for _ in range(num_samples):\n",
    "        # Determine the length of each path randomly, with a minimum of 2 screens\n",
    "        path_length = random.randint(2, len(screens))  # Allows paths of at least 2 screens\n",
    "        path = screens[:path_length]  # Take the ordered sequence up to path_length\n",
    "        navigation_paths.append(path)\n",
    "    return navigation_paths\n",
    "\n",
    "# Initialize the data dictionary with existing fields (excluding 'Debt_To_Income_Ratio')\n",
    "data = {\n",
    "    'User_ID': [fake.uuid4() for _ in range(num_samples)],\n",
    "    'Application_Date': application_dates,\n",
    "    'Age': age,\n",
    "    'Gender': np.random.choice(['Male', 'Female'], size=num_samples),\n",
    "    'Annual_Income': annual_income,\n",
    "    'Credit_Score': credit_score,\n",
    "    'Employment_Status': employment_status,\n",
    "    'Education_Level': education_level,\n",
    "    'Experience': experience,\n",
    "    'Loan_Amount': loan_amount_data,  # Dependency-based Loan_Amount\n",
    "    'Loan_Duration': np.random.choice(\n",
    "        [12, 24, 36, 48, 60, 72, 84, 96, 108, 120], \n",
    "        num_samples, \n",
    "        p=[0.05, 0.1, 0.2, 0.2, 0.2, 0.1, 0.05, 0.05, 0.025, 0.025]\n",
    "    ),\n",
    "    'Marital_Status': np.random.choice(\n",
    "        ['Single', 'Married', 'Divorced', 'Widowed'], \n",
    "        num_samples, \n",
    "        p=[0.3, 0.5, 0.15, 0.05]\n",
    "    ),\n",
    "    'Number_Of_Dependents': np.random.choice(\n",
    "        [0, 1, 2, 3, 4, 5], \n",
    "        num_samples, \n",
    "        p=[0.3, 0.25, 0.2, 0.15, 0.07, 0.03]\n",
    "    ),\n",
    "    'Home_Ownership_Status': np.random.choice(\n",
    "        ['Own', 'Rent', 'Mortgage', 'Other'], \n",
    "        num_samples, \n",
    "        p=[0.2, 0.3, 0.4, 0.1]\n",
    "    ),\n",
    "    'Monthly_Debt_Payments': np.random.lognormal(6, 0.5, num_samples).astype(int),\n",
    "    'Credit_Card_Utilization_Rate': np.random.beta(2, 5, num_samples),\n",
    "    'Number_Of_Open_CreditLines': np.random.poisson(3, num_samples).clip(0, 15).astype(int),\n",
    "    'Number_Of_Credit_Inquiries': np.random.poisson(1, num_samples).clip(0, 10).astype(int),\n",
    "    'Debt_To_Income_Ratio': np.random.beta(2, 5, num_samples),  # Removed 'Debt_To_Income_Ratio'\n",
    "    'Bankruptcy_History': np.random.choice([0, 1], num_samples, p=[0.95, 0.05]),\n",
    "    'Previous_Loan_Defaults': np.random.choice([0, 1], num_samples, p=[0.9, 0.1]),\n",
    "    'Payment_History': np.random.poisson(24, num_samples).clip(0, 60).astype(int),\n",
    "    'Length_Of_CreditHistory': np.random.randint(1, 30, num_samples),\n",
    "    'Savings_Account_Balance': np.random.lognormal(8, 1, num_samples).astype(int),\n",
    "    'Checking_Account_Balance': np.random.lognormal(7, 1, num_samples).astype(int),\n",
    "    'Total_Assets': np.random.lognormal(11, 1, num_samples).astype(int),\n",
    "    'Total_Liabilities': np.random.lognormal(10, 1, num_samples).astype(int),\n",
    "    'Monthly_Income': annual_income / 12,\n",
    "    'Utility_Bills_Payment_History': np.random.beta(8, 2, num_samples),\n",
    "    'Job_Tenure': np.random.poisson(5, num_samples).clip(0, 40).astype(int),\n",
    "    \n",
    "    'Location': location_data,\n",
    "    'Vehicle_Type': vehicle_type_data,\n",
    "    'Vehicle_Make': vehicle_make_data,\n",
    "    'Vehicle_Model': vehicle_model_data,\n",
    "    'Vehicle_Year': vehicle_year_data,\n",
    "    'Vehicle_Mileage': vehicle_mileage_data,\n",
    "    'Vehicle_Price': adjusted_vehicle_price_data,\n",
    "    'Down_Payment': down_payment_data,\n",
    "    'Loan_Tenure_Years': loan_tenure_data,\n",
    "    'Interest_Rate': interest_rate_data,\n",
    "    # 'Application_Status' will be determined via the loan approval function\n",
    "#     'Session_Duration_Minutes': np.random.randint(5, 60, size=num_samples),\n",
    "#     'Number_of_Interactions': np.random.randint(10, 100, size=num_samples),\n",
    "#     'Notifications_Responded': np.random.choice([0, 1], size=num_samples, p=[0.7, 0.3]),\n",
    "#     'Support_Queries': np.random.choice([0, 1, 2, 3], size=num_samples, p=[0.5, 0.3, 0.15, 0.05]),\n",
    "}\n",
    "\n",
    "\n",
    "data.update({\n",
    "    \"Monthly_Expenses\": np.random.randint(1000, 10000, size=num_samples),\n",
    "    \"Previous_Vehicle_Ownership\": np.random.choice([True, False], size=num_samples, p=[0.7, 0.3]),\n",
    "    \"Trade_In_Details\": np.random.choice([None, 'Old Car Trade-In'], size=num_samples, p=[0.7, 0.3]),\n",
    "    \"Session_Start_Time\": [fake.date_time_this_year() for _ in range(num_samples)],\n",
    "    \"Session_End_Time\": [fake.date_time_this_year() for _ in range(num_samples)],\n",
    "    \"Navigation_Paths\": [random.sample(event_sequences, k=random.randint(3, len(event_sequences))) for _ in range(num_samples)],\n",
    "    \n",
    "    \"Device_Type\": np.random.choice(device_types, size=num_samples),\n",
    "    \"OS_Version\": np.random.choice(os_versions, size=num_samples),\n",
    "    \"App_Version\": np.random.choice(app_versions, size=num_samples),\n",
    "    \"Network_Type\": np.random.choice(network_types, size=num_samples),\n",
    "    \"Dealer_Info\": np.random.choice(dealer_info, size=num_samples),\n",
    "    \"Promotions\": np.random.choice(promotions, size=num_samples),\n",
    " \n",
    "    \n",
    "    \"Regulatory_Compliance\": np.random.choice(['Compliant', 'Non-Compliant'], size=num_samples, p=[0.95, 0.05]),\n",
    "    \"Consent_Provided\": np.random.choice([True, False], size=num_samples, p=[0.98, 0.02]),\n",
    "    \"User_Type\": np.random.choice(['New', 'Returning'], size=num_samples),\n",
    "    \"Behavioral_Segment\": np.random.choice(['Low Engagement', 'Medium Engagement', 'High Engagement'], size=num_samples),\n",
    "    \"User_Feedback_Rating\": np.random.randint(1, 5, size=num_samples),\n",
    "    \"Common_Issues_Faced\": np.random.choice(\n",
    "        [None, 'Document Upload Failed', 'Credit Check Issue', 'App Crash'], \n",
    "        size=num_samples, \n",
    "        p=[0.7, 0.1, 0.1, 0.1]\n",
    "    ),\n",
    "    \"User_Satisfaction\": np.random.choice(\n",
    "        ['Very Satisfied', 'Satisfied', 'Neutral', 'Dissatisfied', 'Very Dissatisfied'], \n",
    "        size=num_samples\n",
    "    )\n",
    "})\n",
    "\n",
    "# Additional Interaction Event data to be added\n",
    "data.update({\n",
    "    \"Frequency_of_App_Usage\": np.random.randint(1, 30, size=num_samples),  # Frequency of app usage in the past month\n",
    "    \"Clicks\": np.random.randint(1, 50, size=num_samples),\n",
    "    \"Taps\": np.random.randint(1, 50, size=num_samples),\n",
    "    \"Swipes\": np.random.randint(1, 50, size=num_samples),\n",
    "    \"Form_Entries\": np.random.randint(1, 20, size=num_samples),\n",
    "    \"Time_Spent_on_Home_Screen_Minutes\": np.random.randint(1, 10, size=num_samples),\n",
    "    \"Time_Spent_on_Loan_Calculator_Minutes\": np.random.randint(1, 15, size=num_samples),\n",
    "    \"Time_Spent_on_Vehicle_Selection_Minutes\": np.random.randint(1, 20, size=num_samples),\n",
    "    \"Time_Spent_on_Document_Upload_Minutes\": np.random.randint(1, 10, size=num_samples),\n",
    "    \"Time_Spent_on_Credit_Check_Minutes\": np.random.randint(1, 5, size=num_samples),\n",
    "    \"Time_Spent_on_Approval_Screen_Minutes\": np.random.randint(1, 5, size=num_samples),\n",
    "    #\"Common_Paths\": [screens[:random.randint(3, len(screens))] for _ in range(num_samples)],\n",
    "    \"Drop_Off_Point\": np.random.choice(\n",
    "        screens + [None], \n",
    "        size=num_samples, \n",
    "        p=[0.05, 0.05, 0.2, 0.25, 0.1, 0.3, 0.05]  \n",
    "    ),\n",
    "    \"Comparison_of_Loan_Options\": np.random.choice([True, False], size=num_samples, p=[0.6, 0.4]),\n",
    "    #\"Application_Submitted\": np.random.choice([True, False], size=num_samples, p=[0.8, 0.2])\n",
    "})\n",
    "\n",
    "# Create the DataFrame with all existing and new fields\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Calculate Debt_To_Income_Ratio based on Monthly_Debt_Payments, Monthly_Loan_Payment, and Monthly_Income\n",
    "df['Debt_To_Income_Ratio'] = (\n",
    "    df['Monthly_Debt_Payments'] + df['Loan_Amount'] * (df['Interest_Rate']/100)/12 / (1 - (1 + df['Interest_Rate']/100/12)**(-df['Loan_Tenure_Years']*12))\n",
    ") / df['Monthly_Income']\n",
    "\n",
    "# Create NetWorth ensuring a minimum value\n",
    "min_net_worth = 1000  # Set a minimum net worth\n",
    "df['NetWorth'] = np.maximum(df['Total_Assets'] - df['Total_Liabilities'], min_net_worth)\n",
    "\n",
    "# Calculate Monthly_Loan_Payment using the loan amortization formula\n",
    "# Formula: P = (L * c) / (1 - (1 + c)^-n)\n",
    "# Where:\n",
    "# P = monthly payment\n",
    "# L = loan amount\n",
    "# c = monthly interest rate\n",
    "# n = number of payments\n",
    "\n",
    "df['Monthly_Loan_Payment'] = (\n",
    "    (df['Loan_Amount'] * (df['Interest_Rate']/100) / 12) / \n",
    "    (1 - (1 + df['Interest_Rate']/100 / 12) ** (-df['Loan_Tenure_Years'] * 12))\n",
    ").fillna(0).round(2)\n",
    "\n",
    "# Recalculate Debt_To_Income_Ratio with Monthly_Loan_Payment\n",
    "df['Debt_To_Income_Ratio'] = (\n",
    "    df['Monthly_Debt_Payments'] + df['Monthly_Loan_Payment']\n",
    ") / df['Monthly_Income']\n",
    "\n",
    "# Define a function to calculate approval probability based on multiple factors, with increased weight for DTI and Credit Score\n",
    "def calculate_approval_probability(employment_status, credit_score, dti, loan_amount, vehicle_type, annual_income):\n",
    "    \"\"\"\n",
    "    Calculate the probability of loan approval based on employment status, credit score, DTI, loan amount, vehicle type, and annual income.\n",
    "    Increased weight is given to Credit Score and Debt-To-Income Ratio.\n",
    "    \"\"\"\n",
    "    probability = 0.0\n",
    "    \n",
    "    # Employment Status Factor\n",
    "    if employment_status == 'Employed':\n",
    "        probability += 0.2\n",
    "    elif employment_status == 'Self-Employed':\n",
    "        probability += 0.15\n",
    "    elif employment_status == 'Unemployed':\n",
    "        probability -= 0.25  # Negative impact\n",
    "    \n",
    "    # Credit Score Factor (Increased Weight)\n",
    "    if credit_score >= 750:\n",
    "        probability += 0.35\n",
    "    elif 700 <= credit_score < 750:\n",
    "        probability += 0.25\n",
    "    elif 650 <= credit_score < 700:\n",
    "        probability += 0.15\n",
    "    else:\n",
    "        probability -= 0.35  # Negative impact for low scores\n",
    "    \n",
    "    # Debt-To-Income Ratio Factor (Increased Weight)\n",
    "    if dti <= 0.25:\n",
    "        probability += 0.35\n",
    "    elif 0.25 < dti <= 0.35:\n",
    "        probability += 0.25\n",
    "    elif 0.35 < dti <= 0.45:\n",
    "        probability += 0.15\n",
    "    else:\n",
    "        probability -= 0.35  # Negative impact for high DTI\n",
    "    \n",
    "    # Loan Amount Factor\n",
    "    if loan_amount <= 20000:\n",
    "        probability += 0.15\n",
    "    elif 20000 < loan_amount <= 40000:\n",
    "        probability += 0.1\n",
    "    else:\n",
    "        probability -= 0.25  # Negative impact for very high loans\n",
    "    \n",
    "    # Annual Income Factor\n",
    "    if annual_income >= 100000:\n",
    "        probability += 0.25\n",
    "    elif 75000 <= annual_income < 100000:\n",
    "        probability += 0.2\n",
    "    elif 50000 <= annual_income < 75000:\n",
    "        probability += 0.1\n",
    "    else:\n",
    "        probability -= 0.25  # Negative impact for low income\n",
    "    \n",
    "    # Vehicle Type Factor\n",
    "    if vehicle_type == 'New':\n",
    "        probability += 0.1  # Slightly higher chance for new vehicles\n",
    "    else:\n",
    "        probability += 0.0  # No additional impact for used vehicles\n",
    "    \n",
    "    # Normalize probability to be between 0 and 1\n",
    "    probability = max(min(probability, 1.0), 0.0)\n",
    "    \n",
    "    # Determine Application Status based on probability thresholds\n",
    "    if probability >= 0.75:\n",
    "        status = 'Approved'\n",
    "    elif probability >= 0.45:\n",
    "        status = 'Pending'\n",
    "    else:\n",
    "        status = 'Rejected'\n",
    "    \n",
    "    return status\n",
    "\n",
    "# Apply the loan approval rule to each row using DataFrame.apply\n",
    "df['Loan_Approved'] = df.apply(\n",
    "    lambda row: calculate_approval_probability(\n",
    "        row['Employment_Status'], \n",
    "        row['Credit_Score'], \n",
    "        row['Debt_To_Income_Ratio'], \n",
    "        row['Loan_Amount'], \n",
    "        row['Vehicle_Type'], \n",
    "        row['Annual_Income']\n",
    "    ), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Ensure that if \"Loan_Approved\" is \"Approved\", then \"Drop_Off_Point\" should only show \"Approval\"\n",
    "df.loc[df['Loan_Approved'] == 'Approved', 'Drop_Off_Point'] = 'Approval'\n",
    "\n",
    "# Define possible treatments with a higher probability for \"Ads\" when Drop_Off_Point is \"Approval\"\n",
    "treatments = ['Ads', 'No-Ads']\n",
    "\n",
    "# Create a new column 'Treatment_Assignment' initialized with None\n",
    "df['Treatment_Assignment'] = None\n",
    "\n",
    "# Filter rows where 'Drop_Off_Point' is 'Approval'\n",
    "approval_condition = df['Drop_Off_Point'] == 'Approval'\n",
    "non_approval_condition = df['Drop_Off_Point'].isin(['Document Upload', 'Credit Check'])\n",
    "\n",
    "# Assign treatments with higher probability for \"Ads\" when 'Drop_Off_Point' is 'Approval'\n",
    "df.loc[approval_condition, 'Treatment_Assignment'] = np.random.choice(\n",
    "    treatments, size=approval_condition.sum(), p=[0.8, 0.2]\n",
    ")\n",
    "\n",
    "# Assign random treatment to the non-approval filtered rows\n",
    "df.loc[non_approval_condition, 'Treatment_Assignment'] = np.random.choice(\n",
    "    treatments, size=non_approval_condition.sum(), p=[0.5, 0.5]\n",
    ")\n",
    "\n",
    "# Assign 'No-Ads' to all other rows where 'Treatment_Assignment' is still None\n",
    "df['Treatment_Assignment'].fillna('No-Ads', inplace=True)\n",
    "\n",
    "# Ensure Total_Assets is always greater than or equal to the sum of Savings_Account_Balance and Checking_Account_Balance\n",
    "df['Total_Assets'] = np.maximum(df['Total_Assets'], df['Savings_Account_Balance'] + df['Checking_Account_Balance'])\n",
    "\n",
    "# Add more complex derived features\n",
    "df['Net_Worth'] = np.maximum(df['Total_Assets'] - df['Total_Liabilities'], min_net_worth)\n",
    "\n",
    "# Add some noise and outliers\n",
    "noise_mask = np.random.choice([True, False], num_samples, p=[0.01, 0.99])\n",
    "df.loc[noise_mask, 'Annual_Income'] = (\n",
    "    df.loc[noise_mask, 'Annual_Income'] * np.random.uniform(1.5, 2.0, noise_mask.sum())\n",
    ").astype(int)\n",
    "\n",
    "low_net_worth_mask = df['Net_Worth'] == min_net_worth\n",
    "df.loc[low_net_worth_mask, 'Net_Worth'] += np.random.randint(0, 10000, size=low_net_worth_mask.sum())\n",
    "\n",
    "# Save the updated DataFrame to a CSV file\n",
    "csv_file_path = \"Synthetic_Auto_Loan_Application_Data_jz3.csv\"\n",
    "df.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cf28e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
